
C:\Users\User\git\ML-Ops-Final\prob_slices_using_decision_trees>python main.py otherconfig.json
Loaded config.json:
{'validation_size': 0.15, 'test_size': 0.15, 'min_support': 0.05, 'iterations': 8, 'dataset_path': './datasets/german.data', 'dataset_contains_column_names': False, 'dataset_column_names': ['existingchecking', 'duration', 'credithistory', 'purpose', 'creditamount', 'savings', 'employmentsince', 'installmentrate', 'statussex', 'otherdebtors', 'residencesince', 'property', 'age', 'otherinstallmentplans', 'housing', 'existingcredits', 'job', 'peopleliable', 'telephone', 'foreignworker', 'classification'], 'dataset_label_column_name': 'classification', 'dataset_label_values': [1, 2], 'dataset_label_replace_with': [1, 0], 'problematic_label': 0, 'synthetic_samples_to_generate_percent': 1, 'should_use_ctgan': False, 'ctgan_num_epochs': 10, 'metric_to_use_name': 'f1', 'should_generate_data_from_both_labels': False}
Starting...
Completed Dataset Loading
Categorical features:
['existingchecking', 'credithistory', 'purpose', 'savings', 'employmentsince', 'statussex', 'otherdebtors', 'property', 'otherinstallmentplans', 'housing', 'job', 'telephone', 'foreignworker']
Split train/test/val
Done preprocessing
Starting iteration #1
[19:27:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
Fit XGBClassifier
Regular Baseline Model Classification Report On Validation Dataset:
              precision    recall  f1-score   support

           0     0.6333    0.4222    0.5067        45
           1     0.7833    0.8952    0.8356       105

    accuracy                         0.7533       150
   macro avg     0.7083    0.6587    0.6711       150
weighted avg     0.7383    0.7533    0.7369       150

Regular Baseline Model Classification Report On Test Dataset:
              precision    recall  f1-score   support

           0     0.5000    0.4444    0.4706        45
           1     0.7727    0.8095    0.7907       105

    accuracy                         0.7000       150
   macro avg     0.6364    0.6270    0.6306       150
weighted avg     0.6909    0.7000    0.6947       150

Accuracy threshold: 0.8355555555555555
Created Decision Tree Classifier for finding problematic slices
[('duration', 27.0), ('creditamount', 1220.0), ('undefined!', -2.0), ('undefined!', -2.0), ('property_A121', 0.5), ('undefined!', -2.0), ('undefined!', -2.0)]
Slices of tree filter descriptions:
[('duration', '<=', 27.0), ('creditamount', '<=', 1220.0)]
[('duration', '<=', 27.0), ('creditamount', '>', 1220.0)]
[('duration', '>', 27.0), ('property_A121', '<=', 0.5)]
[('duration', '>', 27.0), ('property_A121', '>', 0.5)]
Extracted the slices by the tree leaves
Accuracy of slice #1 is: 0.0
Accuracy of slice #2 is: 0.0
Accuracy of slice #3 is: 0.0
Accuracy of slice #4 is: 0.0
Calculated accuracy of each slice
Min size threshold: 2.25
Slice #1, Accuracy of 0.0 and size of 4
Slice #2, Accuracy of 0.0 and size of 29
Slice #3, Accuracy of 0.0 and size of 10
Slice #4, Accuracy of 0.0 and size of 2
Most problematic slice index: 1
Chose the most problematic slice
Filter #1: duration <= 27.0
Filter #2: creditamount <= 1220.0
Train problematic slice size: 125
Extracted the problematic slice from the train dataframe
Train problematic slice size after label filer: 39
Filtered the problematic slice from the train dataframe
Training CTGAN on problematic slice
Generated 39 new samples for the problematic slice
Generated new samples from the problematic slice
[19:27:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
Fit XGBClassifier
Accuracy of slice #1 is: 0.0
Accuracy of slice #2 is: 0.0
Accuracy of slice #3 is: 0.0
Accuracy of slice #4 is: 0.0
Finished cycle for label 0
Added new samples to the train dataframe
going back...
Starting iteration #2
[19:27:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
Fit XGBClassifier
Regular Baseline Model Classification Report On Validation Dataset:
              precision    recall  f1-score   support

           0     0.5517    0.3556    0.4324        45
           1     0.7603    0.8762    0.8142       105

    accuracy                         0.7200       150
   macro avg     0.6560    0.6159    0.6233       150
weighted avg     0.6977    0.7200    0.6996       150

Regular Baseline Model Classification Report On Test Dataset:
              precision    recall  f1-score   support

           0     0.5122    0.4667    0.4884        45
           1     0.7798    0.8095    0.7944       105

    accuracy                         0.7067       150
   macro avg     0.6460    0.6381    0.6414       150
weighted avg     0.6995    0.7067    0.7026       150

Accuracy threshold: 0.8355555555555555
Created Decision Tree Classifier for finding problematic slices
[('creditamount', 1220.0), ('undefined!', -2.0), ('duration', 22.5), ('undefined!', -2.0), ('undefined!', -2.0)]
Slices of tree filter descriptions:
[('creditamount', '<=', 1220.0), ('undefined!', '<=', -2.0)]
[('creditamount', '<=', 1220.0), ('undefined!', '>', -2.0)]
[('creditamount', '>', 1220.0), ('undefined!', '<=', -2.0)]
[('creditamount', '>', 1220.0), ('undefined!', '>', -2.0)]
Extracted the slices by the tree leaves
Accuracy of slice #1 is: 0.0
Accuracy of slice #2 is: 0.0
Accuracy of slice #3 is: 0.0
Accuracy of slice #4 is: 0.0
Calculated accuracy of each slice
Min size threshold: 2.25
Slice #1, Accuracy of 0.0 and size of 4
Slice #2, Accuracy of 0.0 and size of 4
Slice #3, Accuracy of 0.0 and size of 41
Slice #4, Accuracy of 0.0 and size of 41
Most problematic slice index: 1
Chose the most problematic slice
Filter #1: creditamount <= 1220.0
Filter #2: undefined! <= -2.0
Train problematic slice size: 165
Extracted the problematic slice from the train dataframe
Train problematic slice size after label filer: 78
Filtered the problematic slice from the train dataframe
Training CTGAN on problematic slice
Generated 78 new samples for the problematic slice
Generated new samples from the problematic slice
[19:27:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
Fit XGBClassifier
Accuracy of slice #1 is: 0.0
Accuracy of slice #2 is: 0.0
Accuracy of slice #3 is: 0.0
Accuracy of slice #4 is: 0.0
Finished cycle for label 0
Added new samples to the train dataframe
going back...
Starting iteration #3
[19:27:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
Fit XGBClassifier
Regular Baseline Model Classification Report On Validation Dataset:
              precision    recall  f1-score   support

           0     0.5625    0.4000    0.4675        45
           1     0.7712    0.8667    0.8161       105

    accuracy                         0.7267       150
   macro avg     0.6668    0.6333    0.6418       150
weighted avg     0.7086    0.7267    0.7116       150

Regular Baseline Model Classification Report On Test Dataset:
              precision    recall  f1-score   support

           0     0.5366    0.4889    0.5116        45
           1     0.7890    0.8190    0.8037       105

    accuracy                         0.7200       150
   macro avg     0.6628    0.6540    0.6577       150
weighted avg     0.7133    0.7200    0.7161       150

Accuracy threshold: 0.8355555555555555
Created Decision Tree Classifier for finding problematic slices
[('creditamount', 1260.0), ('existingcredits', 1.5), ('undefined!', -2.0), ('undefined!', -2.0), ('duration', 22.5), ('undefined!', -2.0), ('undefined!', -2.0)]
Slices of tree filter descriptions:
[('creditamount', '<=', 1260.0), ('existingcredits', '<=', 1.5)]
[('creditamount', '<=', 1260.0), ('existingcredits', '>', 1.5)]
[('creditamount', '>', 1260.0), ('duration', '<=', 22.5)]
[('creditamount', '>', 1260.0), ('duration', '>', 22.5)]
Extracted the slices by the tree leaves
Accuracy of slice #1 is: 0.0
Accuracy of slice #2 is: 0.0
Accuracy of slice #3 is: 0.0
Accuracy of slice #4 is: 0.0
Calculated accuracy of each slice
Min size threshold: 2.25
Slice #1, Accuracy of 0.0 and size of 6
Slice #2, Accuracy of 0.0 and size of 1
Slice #3, Accuracy of 0.0 and size of 12
Slice #4, Accuracy of 0.0 and size of 26
Most problematic slice index: 1
Chose the most problematic slice
Filter #1: creditamount <= 1260.0
Filter #2: existingcredits <= 1.5
Train problematic slice size: 175
Extracted the problematic slice from the train dataframe
Train problematic slice size after label filer: 123
Filtered the problematic slice from the train dataframe
Training CTGAN on problematic slice
Generated 123 new samples for the problematic slice
Generated new samples from the problematic slice
[19:27:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
Fit XGBClassifier
Accuracy of slice #1 is: 0.0
Accuracy of slice #2 is: 0.0
Accuracy of slice #3 is: 0.0
Accuracy of slice #4 is: 0.0
Finished cycle for label 0
Added new samples to the train dataframe
going back...
Starting iteration #4
[19:27:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
Fit XGBClassifier
Regular Baseline Model Classification Report On Validation Dataset:
              precision    recall  f1-score   support

           0     0.5294    0.4000    0.4557        45
           1     0.7672    0.8476    0.8054       105

    accuracy                         0.7133       150
   macro avg     0.6483    0.6238    0.6306       150
weighted avg     0.6959    0.7133    0.7005       150

Regular Baseline Model Classification Report On Test Dataset:
              precision    recall  f1-score   support

           0     0.5111    0.5111    0.5111        45
           1     0.7905    0.7905    0.7905       105

    accuracy                         0.7067       150
   macro avg     0.6508    0.6508    0.6508       150
weighted avg     0.7067    0.7067    0.7067       150

Accuracy threshold: 0.8355555555555555
Created Decision Tree Classifier for finding problematic slices
[('creditamount', 1220.0), ('undefined!', -2.0), ('duration', 22.5), ('undefined!', -2.0), ('undefined!', -2.0)]
Slices of tree filter descriptions:
[('creditamount', '<=', 1220.0), ('undefined!', '<=', -2.0)]
[('creditamount', '<=', 1220.0), ('undefined!', '>', -2.0)]
[('creditamount', '>', 1220.0), ('undefined!', '<=', -2.0)]
[('creditamount', '>', 1220.0), ('undefined!', '>', -2.0)]
Extracted the slices by the tree leaves
Accuracy of slice #1 is: 0.0
Accuracy of slice #2 is: 0.0
Accuracy of slice #3 is: 0.0
Accuracy of slice #4 is: 0.0
Calculated accuracy of each slice
Min size threshold: 2.25
Slice #1, Accuracy of 0.0 and size of 4
Slice #2, Accuracy of 0.0 and size of 4
Slice #3, Accuracy of 0.0 and size of 41
Slice #4, Accuracy of 0.0 and size of 41
Most problematic slice index: 1
Chose the most problematic slice
Filter #1: creditamount <= 1220.0
Filter #2: undefined! <= -2.0
Train problematic slice size: 363
Extracted the problematic slice from the train dataframe
Train problematic slice size after label filer: 276
Filtered the problematic slice from the train dataframe
Training CTGAN on problematic slice
Generated 276 new samples for the problematic slice
Generated new samples from the problematic slice
[19:27:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
Fit XGBClassifier
Accuracy of slice #1 is: 0.0
Accuracy of slice #2 is: 0.0
Accuracy of slice #3 is: 0.0
Accuracy of slice #4 is: 0.0
Finished cycle for label 0
Added new samples to the train dataframe
going back...
Starting iteration #5
[19:27:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
Fit XGBClassifier
Regular Baseline Model Classification Report On Validation Dataset:
              precision    recall  f1-score   support

           0     0.5405    0.4444    0.4878        45
           1     0.7788    0.8381    0.8073       105

    accuracy                         0.7200       150
   macro avg     0.6597    0.6413    0.6476       150
weighted avg     0.7073    0.7200    0.7115       150

Regular Baseline Model Classification Report On Test Dataset:
              precision    recall  f1-score   support

           0     0.5238    0.4889    0.5057        45
           1     0.7870    0.8095    0.7981       105

    accuracy                         0.7133       150
   macro avg     0.6554    0.6492    0.6519       150
weighted avg     0.7081    0.7133    0.7104       150

Accuracy threshold: 0.8355555555555555
Created Decision Tree Classifier for finding problematic slices
[('property_A121', 0.5), ('employmentsince_A71', 0.5), ('undefined!', -2.0), ('undefined!', -2.0), ('creditamount', 1237.0), ('undefined!', -2.0), ('undefined!', -2.0)]
Slices of tree filter descriptions:
[('property_A121', '<=', 0.5), ('employmentsince_A71', '<=', 0.5)]
[('property_A121', '<=', 0.5), ('employmentsince_A71', '>', 0.5)]
[('property_A121', '>', 0.5), ('creditamount', '<=', 1237.0)]
[('property_A121', '>', 0.5), ('creditamount', '>', 1237.0)]
Extracted the slices by the tree leaves
Accuracy of slice #1 is: 0.0
Accuracy of slice #2 is: 0.0
Accuracy of slice #3 is: 0.0
Accuracy of slice #4 is: 0.0
Calculated accuracy of each slice
Min size threshold: 2.25
Slice #1, Accuracy of 0.0 and size of 27
Slice #2, Accuracy of 0.0 and size of 4
Slice #3, Accuracy of 0.0 and size of 3
Slice #4, Accuracy of 0.0 and size of 11
Most problematic slice index: 1
Chose the most problematic slice
Filter #1: property_A121 <= 0.5
Filter #2: employmentsince_A71 <= 0.5
Train problematic slice size: 818
Extracted the problematic slice from the train dataframe
Train problematic slice size after label filer: 503
Filtered the problematic slice from the train dataframe
Training CTGAN on problematic slice
Generated 503 new samples for the problematic slice
Generated new samples from the problematic slice
[19:27:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
Fit XGBClassifier
Accuracy of slice #1 is: 0.0
Accuracy of slice #2 is: 0.0
Accuracy of slice #3 is: 0.0
Accuracy of slice #4 is: 0.0
Finished cycle for label 0
Added new samples to the train dataframe
going back...
Starting iteration #6
[19:27:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
Fit XGBClassifier
Regular Baseline Model Classification Report On Validation Dataset:
              precision    recall  f1-score   support

           0     0.5135    0.4222    0.4634        45
           1     0.7699    0.8286    0.7982       105

    accuracy                         0.7067       150
   macro avg     0.6417    0.6254    0.6308       150
weighted avg     0.6930    0.7067    0.6977       150

Regular Baseline Model Classification Report On Test Dataset:
              precision    recall  f1-score   support

           0     0.4889    0.4889    0.4889        45
           1     0.7810    0.7810    0.7810       105

    accuracy                         0.6933       150
   macro avg     0.6349    0.6349    0.6349       150
weighted avg     0.6933    0.6933    0.6933       150

Accuracy threshold: 0.8355555555555555
Created Decision Tree Classifier for finding problematic slices
[('duration', 27.0), ('duration', 16.5), ('undefined!', -2.0), ('undefined!', -2.0), ('telephone_A192', 0.5), ('undefined!', -2.0), ('undefined!', -2.0)]
Slices of tree filter descriptions:
[('duration', '<=', 27.0), ('duration', '<=', 16.5)]
[('duration', '<=', 27.0), ('duration', '>', 16.5)]
[('duration', '>', 27.0), ('telephone_A192', '<=', 0.5)]
[('duration', '>', 27.0), ('telephone_A192', '>', 0.5)]
Extracted the slices by the tree leaves
Accuracy of slice #1 is: 0.0
Accuracy of slice #2 is: 0.0
Accuracy of slice #3 is: 0.0
Accuracy of slice #4 is: 0.0
Calculated accuracy of each slice
Min size threshold: 2.25
Slice #1, Accuracy of 0.0 and size of 11
Slice #2, Accuracy of 0.0 and size of 22
Slice #3, Accuracy of 0.0 and size of 9
Slice #4, Accuracy of 0.0 and size of 3
Most problematic slice index: 1
Chose the most problematic slice
Filter #1: duration <= 27.0
Filter #2: duration <= 16.5
Train problematic slice size: 972
Extracted the problematic slice from the train dataframe
Train problematic slice size after label filer: 731
Filtered the problematic slice from the train dataframe
Training CTGAN on problematic slice
Generated 731 new samples for the problematic slice
Generated new samples from the problematic slice
[19:27:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
Fit XGBClassifier
Accuracy of slice #1 is: 0.0
Accuracy of slice #2 is: 0.0
Accuracy of slice #3 is: 0.0
Accuracy of slice #4 is: 0.0
Finished cycle for label 0
Added new samples to the train dataframe
going back...
Starting iteration #7
[19:27:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
Fit XGBClassifier
Regular Baseline Model Classification Report On Validation Dataset:
              precision    recall  f1-score   support

           0     0.5882    0.4444    0.5063        45
           1     0.7845    0.8667    0.8235       105

    accuracy                         0.7400       150
   macro avg     0.6864    0.6556    0.6649       150
weighted avg     0.7256    0.7400    0.7284       150

Regular Baseline Model Classification Report On Test Dataset:
              precision    recall  f1-score   support

           0     0.4783    0.4889    0.4835        45
           1     0.7788    0.7714    0.7751       105

    accuracy                         0.6867       150
   macro avg     0.6286    0.6302    0.6293       150
weighted avg     0.6887    0.6867    0.6876       150

Accuracy threshold: 0.8355555555555555
Created Decision Tree Classifier for finding problematic slices
[('duration', 22.5), ('creditamount', 1220.0), ('undefined!', -2.0), ('undefined!', -2.0), ('age', 29.5), ('undefined!', -2.0), ('undefined!', -2.0)]
Slices of tree filter descriptions:
[('duration', '<=', 22.5), ('creditamount', '<=', 1220.0)]
[('duration', '<=', 22.5), ('creditamount', '>', 1220.0)]
[('duration', '>', 22.5), ('age', '<=', 29.5)]
[('duration', '>', 22.5), ('age', '>', 29.5)]
Extracted the slices by the tree leaves
Accuracy of slice #1 is: 0.0
Accuracy of slice #2 is: 0.0
Accuracy of slice #3 is: 0.0
Accuracy of slice #4 is: 0.0
Calculated accuracy of each slice
Min size threshold: 2.25
Slice #1, Accuracy of 0.0 and size of 3
Slice #2, Accuracy of 0.0 and size of 14
Slice #3, Accuracy of 0.0 and size of 15
Slice #4, Accuracy of 0.0 and size of 13
Most problematic slice index: 1
Chose the most problematic slice
Filter #1: duration <= 22.5
Filter #2: creditamount <= 1220.0
Train problematic slice size: 1545
Extracted the problematic slice from the train dataframe
Train problematic slice size after label filer: 1464
Filtered the problematic slice from the train dataframe
Training CTGAN on problematic slice
Generated 1464 new samples for the problematic slice
Generated new samples from the problematic slice
[19:27:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
Fit XGBClassifier
Accuracy of slice #1 is: 0.0
Accuracy of slice #2 is: 0.0
Accuracy of slice #3 is: 0.0
Accuracy of slice #4 is: 0.0
Finished cycle for label 0
Added new samples to the train dataframe
going back...
Starting iteration #8
[19:27:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
Fit XGBClassifier
Regular Baseline Model Classification Report On Validation Dataset:
              precision    recall  f1-score   support

           0     0.5588    0.4222    0.4810        45
           1     0.7759    0.8571    0.8145       105

    accuracy                         0.7267       150
   macro avg     0.6673    0.6397    0.6477       150
weighted avg     0.7108    0.7267    0.7144       150

Regular Baseline Model Classification Report On Test Dataset:
              precision    recall  f1-score   support

           0     0.4600    0.5111    0.4842        45
           1     0.7800    0.7429    0.7610       105

    accuracy                         0.6733       150
   macro avg     0.6200    0.6270    0.6226       150
weighted avg     0.6840    0.6733    0.6779       150

Accuracy threshold: 0.8355555555555555
Created Decision Tree Classifier for finding problematic slices
[('creditamount', 1260.0), ('housing_A152', 0.5), ('undefined!', -2.0), ('undefined!', -2.0), ('duration', 19.5), ('undefined!', -2.0), ('undefined!', -2.0)]
Slices of tree filter descriptions:
[('creditamount', '<=', 1260.0), ('housing_A152', '<=', 0.5)]
[('creditamount', '<=', 1260.0), ('housing_A152', '>', 0.5)]
[('creditamount', '>', 1260.0), ('duration', '<=', 19.5)]
[('creditamount', '>', 1260.0), ('duration', '>', 19.5)]
Extracted the slices by the tree leaves
Accuracy of slice #1 is: 0.0
Accuracy of slice #2 is: 0.0
Accuracy of slice #3 is: 0.0
Accuracy of slice #4 is: 0.0
Calculated accuracy of each slice
Min size threshold: 2.25
Slice #1, Accuracy of 0.0 and size of 1
Slice #2, Accuracy of 0.0 and size of 6
Slice #3, Accuracy of 0.0 and size of 11
Slice #4, Accuracy of 0.0 and size of 27
Most problematic slice index: 2
Chose the most problematic slice
Filter #1: creditamount <= 1260.0
Filter #2: housing_A152 > 0.5
Train problematic slice size: 2457
Extracted the problematic slice from the train dataframe
Train problematic slice size after label filer: 2376
Filtered the problematic slice from the train dataframe
Training CTGAN on problematic slice
Generated 2376 new samples for the problematic slice
Generated new samples from the problematic slice
[19:27:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
Fit XGBClassifier
Accuracy of slice #1 is: 0.0
Accuracy of slice #2 is: 0.0
Accuracy of slice #3 is: 0.0
Accuracy of slice #4 is: 0.0
Finished cycle for label 0
Added new samples to the train dataframe
going back...
Finished all iterations, printing final models results:
[19:27:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
Fit XGBClassifier
Regular Baseline Model Classification Report On Validation Dataset:
              precision    recall  f1-score   support

           0     0.6061    0.4444    0.5128        45
           1     0.7863    0.8762    0.8288       105

    accuracy                         0.7467       150
   macro avg     0.6962    0.6603    0.6708       150
weighted avg     0.7322    0.7467    0.7340       150

Regular Baseline Model Classification Report On Test Dataset:
              precision    recall  f1-score   support

           0     0.5400    0.6000    0.5684        45
           1     0.8200    0.7810    0.8000       105

    accuracy                         0.7267       150
   macro avg     0.6800    0.6905    0.6842       150
weighted avg     0.7360    0.7267    0.7305       150

Dumped the best model into file
Dumped the last model into file, exiting...

C:\Users\User\git\ML-Ops-Final\prob_slices_using_decision_trees>